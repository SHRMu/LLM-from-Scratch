# Chapter 2: The Transformer Architecture in Detail

Welcome to the second chapter! Here, we will build the foundational understanding of how a Large Language Model (LLM) processes text by diving deep into the components of the Transformer architecture.

This chapter is divided into the following articles:

1.  [Tokenization: The Vocabulary of Machines](./01_tokenization.md)
2.  [Embeddings: Translating Tokens into Meaning](./02_embeddings.md)
3.  [The Attention Mechanism: How Models Focus](./03_attention.md)
4.  [Hands-on Guide: Calculating Attention](./04_hands_on_guide_attention.md)

---
### Related Ideas & Notes

```dataview
TABLE status, tags
FROM "self-thinking"
WHERE contains(related_chapters, 2)
```