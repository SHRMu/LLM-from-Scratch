# Chapter 1: The Evolution of Sequence Models

To truly understand the Transformer, we must first journey through the architectures that came before it. This chapter explores the history of models designed to process sequential data like text, highlighting their innovations and, crucially, the limitations that led to the development of the Transformer.

This chapter is divided into the following articles:

1.  [Recurrent Neural Networks (RNNs)](./01_recurrent_neural_networks.md)
2.  [Long Short-Term Memory (LSTMs) & GRUs](./02_long_short_term_memory.md)
3.  [Brief Interlude: Convolutional Neural Networks (CNNs) in NLP](./03_cnns_in_nlp.md)
4.  [The Bottleneck: Limitations of Sequential Processing](./04_limitations_of_sequential_processing.md)
5.  [Hands-on Guide: Building a Simple RNN](./05_hands_on_guide_rnn.md)

---
### Related Ideas & Notes

```dataview
TABLE status, tags
FROM "self-thinking"
WHERE contains(related_chapters, 1)
```