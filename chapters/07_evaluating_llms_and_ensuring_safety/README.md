# Chapter 7: Evaluating LLMs and Ensuring Safety

Building and using a Large Language Model is only half the battle. How do we know if a model is good? How do we ensure it's safe and reliable? This chapter covers the essential practices of evaluating LLM performance and addressing the critical challenges of alignment, bias, and security.

This chapter is divided into the following articles:

1.  [How to Evaluate LLMs](./01_how_to_evaluate_llms.md)
2.  [The Alignment Problem](./02_the_alignment_problem.md)
3.  [Bias, Fairness, and Misinformation](./03_bias_fairness_and_misinformation.md)
4.  [Red Teaming and Security](./04_red_teaming_and_security.md)
5.  [Hands-on Guide: Running an Evaluation Benchmark](./05_hands_on_guide_evaluation.md)

---
### Related Ideas & Notes

```dataview
TABLE status, tags
FROM "self-thinking"
WHERE contains(related_chapters, 7)
```