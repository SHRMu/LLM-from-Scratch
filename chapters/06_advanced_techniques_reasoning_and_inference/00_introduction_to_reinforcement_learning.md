# Introduction to Reinforcement Learning

Reinforcement Learning (RL) is a machine learning paradigm where an **agent** learns to make decisions by interacting with an **environment**. The agent's goal is to maximize a cumulative **reward** signal it receives from the environment. Unlike supervised learning, where the agent is told what to do, in RL, the agent must discover which actions yield the most reward through trial and error.

### Core Components of Reinforcement Learning

Every RL problem can be broken down into a few key components:

1.  **Agent:** The learner or decision-maker. In the context of LLMs, the agent is the model itself.
2.  **Environment:** The external world with which the agent interacts. It takes the agent's current **state** and **action** as input and returns a new **state** and a **reward** as output.
3.  **State (`s`):** A snapshot of the environment at a particular moment. It contains all the information the agent needs to make a decision.
4.  **Action (`a`):** A move the agent can make in the environment. The set of all possible moves is called the **action space**.
5.  **Reward (`r`):** A scalar feedback signal from the environment that indicates how well the agent is doing. The agent's objective is to maximize the total reward over time.
6.  **Policy (`π`):** The agent's strategy for choosing actions given a particular state. It's a mapping from states to actions.
    *   A **deterministic policy** is a direct mapping: `a = π(s)`.
    *   A **stochastic policy** is a probability distribution over actions: `π(a|s) = P(A=a | S=s)`.
7.  **Value Function (`V(s)` or `Q(s,a)`):** A prediction of the future reward. It estimates how good it is for the agent to be in a particular state (or to take a particular action in a state).
    *   **State-Value Function (`V(s)`):** The expected cumulative reward starting from state `s` and following policy `π`.
    *   **Action-Value Function (`Q(s,a)`):** The expected cumulative reward from taking action `a` in state `s` and then following policy `π`.

### The Learning Loop

The interaction between the agent and the environment creates a feedback loop:

1.  The agent observes the current **state** (`s_t`) of the environment.
2.  Based on its **policy** (`π`), the agent chooses an **action** (`a_t`).
3.  The environment executes the action and transitions to a new **state** (`s_{t+1}`).
4.  The environment provides a **reward** (`r_{t+1}`) to the agent.
5.  The agent updates its policy and value function based on the reward and the new state.

This loop continues until the environment reaches a terminal state, completing an **episode**. The agent's goal is to learn a policy that maximizes the cumulative reward over many episodes.

### The Exploration-Exploitation Trade-off

A fundamental challenge in RL is the trade-off between **exploration** and **exploitation**:

-   **Exploitation:** The agent makes the best decision it can, given its current knowledge of the environment. It chooses the action that it believes will yield the highest reward.
-   **Exploration:** The agent tries a new, random action to see what happens. This allows it to discover new states and potentially find better rewards in the long run.

The agent must strike a balance between these two. Too much exploitation, and it might get stuck in a suboptimal strategy. Too much exploration, and it will never capitalize on its knowledge.

### Connection to LLMs and RLHF

In the context of RLHF, this framework is adapted to align LLMs with human preferences:

-   **Agent:** The LLM.
-   **Environment:** The user and the Reward Model.
-   **State:** The current conversation history or prompt.
-   **Action:** The next response generated by the LLM.
-   **Reward:** The score given by the Reward Model, which was trained on human preference data.

By using RL, the LLM learns to generate responses that are more likely to receive a high reward, effectively aligning its behavior with what humans consider to be "good" answers.
